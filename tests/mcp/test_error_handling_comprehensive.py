"""Comprehensive error handling and edge case tests for MCP tools.

This module tests error scenarios, edge cases, and failure conditions to ensure
robust MCP tool behavior under adverse conditions.
"""

import asyncio
import json
import pytest
import time
import random
from typing import Dict, Any, List, Optional
from unittest.mock import patch, AsyncMock, Mock
from concurrent.futures import ThreadPoolExecutor

from tests.mcp.helpers import mcp_test_server
from tests.mcp.test_data import WorkflowTestData
from tests.mcp.mocks import MockScenarios
from tests.mcp.mock_infrastructure_enhanced import (
    StatefulMockClient,
    NetworkSimulator,
    EnhancedMockScenarios
)


class TestNetworkFailureHandling:
    """Test network failure scenarios and recovery mechanisms."""
    
    @pytest.mark.asyncio
    async def test_connection_timeout_handling(self):
        """Test handling of connection timeouts."""
        async with mcp_test_server(debug=True) as server:
            with patch('kubiya_workflow_sdk.client.StreamingKubiyaClient') as mock_client_class:
                mock_client = AsyncMock()
                mock_client_class.return_value = mock_client
                
                # Simulate connection timeout
                async def timeout_response(*args, **kwargs):
                    await asyncio.sleep(10)  # Longer than typical timeout
                    return {"status": "timeout"}
                
                mock_client.compile_workflow.side_effect = timeout_response
                
                try:
                    # This should timeout or handle gracefully
                    result = await asyncio.wait_for(
                        server.call_tool("compile_workflow", {
                            "dsl_code": WorkflowTestData.SIMPLE_WORKFLOWS["hello_world"]["dsl"]
                        }),
                        timeout=2.0
                    )
                    
                    # Should handle timeout gracefully
                    if result and result.get("isError", False):
                        print("‚úÖ Connection timeout handled gracefully")
                    else:
                        print("üìù Timeout handling behavior unclear")
                        
                except asyncio.TimeoutError:
                    print("‚úÖ Connection timeout properly detected and handled")
                except Exception as e:
                    print(f"‚úÖ Connection timeout caused controlled exception: {e}")

    @pytest.mark.asyncio
    async def test_connection_refused_handling(self):
        """Test handling of connection refused errors."""
        async with mcp_test_server(debug=True) as server:
            with patch('kubiya_workflow_sdk.client.StreamingKubiyaClient') as mock_client_class:
                mock_client = AsyncMock()
                mock_client_class.return_value = mock_client
                
                # Simulate connection refused
                mock_client.get_runners.side_effect = ConnectionError("Connection refused")
                
                try:
                    result = await server.call_tool("get_workflow_runners", {})
                    
                    # Should handle connection error gracefully
                    if result and result.get("isError", False):
                        error_content = str(result.get("content", "")).lower()
                        if "connection" in error_content or "network" in error_content:
                            print("‚úÖ Connection refused error properly handled")
                        else:
                            print("üìù Connection error handling unclear")
                    else:
                        print("üìù Connection refused not properly detected")
                        
                except Exception as e:
                    print(f"‚úÖ Connection refused caused controlled exception: {e}")

    @pytest.mark.asyncio
    async def test_intermittent_connectivity_resilience(self):
        """Test resilience to intermittent connectivity issues."""
        async with mcp_test_server(debug=True) as server:
            with patch('kubiya_workflow_sdk.client.StreamingKubiyaClient') as mock_client_class:
                # Use intermittent failure mock
                mock_client = EnhancedMockScenarios.create_intermittent_failure_scenario()
                mock_client_class.return_value = mock_client
                
                success_count = 0
                failure_count = 0
                
                # Test multiple requests to verify intermittent behavior
                for i in range(8):
                    try:
                        result = await server.call_tool("get_workflow_runners", {
                            "refresh": True
                        })
                        
                        if result and not result.get("isError", False):
                            success_count += 1
                        else:
                            failure_count += 1
                            
                    except Exception:
                        failure_count += 1
                    
                    await asyncio.sleep(0.1)
                
                print(f"‚úÖ Intermittent connectivity test: {success_count} successes, {failure_count} failures")
                
                # Should have both successes and failures for intermittent scenario
                assert success_count + failure_count == 8

    @pytest.mark.asyncio
    async def test_dns_resolution_failure(self):
        """Test handling of DNS resolution failures."""
        async with mcp_test_server(debug=True) as server:
            with patch('kubiya_workflow_sdk.client.StreamingKubiyaClient') as mock_client_class:
                mock_client = AsyncMock()
                mock_client_class.return_value = mock_client
                
                # Simulate DNS resolution failure
                mock_client.get_integrations.side_effect = OSError("Name or service not known")
                
                try:
                    result = await server.call_tool("get_integrations", {
                        "category": "communication"
                    })
                    
                    # Should handle DNS error gracefully
                    if result and result.get("isError", False):
                        print("‚úÖ DNS resolution failure handled gracefully")
                    else:
                        print("üìù DNS failure not properly detected")
                        
                except Exception as e:
                    print(f"‚úÖ DNS failure caused controlled exception: {e}")


class TestInvalidWorkflowHandling:
    """Test handling of invalid workflow definitions and malformed inputs."""
    
    @pytest.mark.asyncio
    async def test_invalid_yaml_syntax(self):
        """Test handling of invalid YAML syntax in workflow DSL."""
        invalid_yaml_cases = [
            "name: test\n  invalid_indent:",
            "name: test\nsteps:\n  - name: step\n    run: echo 'test'\n  invalid: yaml",
            "name: [invalid yaml structure",
            "name: test\nsteps:\n  - name:\n    run:",  # Missing values
            "---\ninvalid: yaml: structure: nested:",
        ]
        
        async with mcp_test_server(debug=True) as server:
            with patch('kubiya_workflow_sdk.client.StreamingKubiyaClient') as mock_client_class:
                mock_client = AsyncMock()
                mock_client_class.return_value = mock_client
                
                # Configure mock to return validation errors for invalid DSL
                def mock_compile_response(dsl_code, **kwargs):
                    return {
                        "workflow_id": None,
                        "status": "failed",
                        "validation_errors": ["Invalid YAML syntax", "DSL validation failed"],
                        "compilation_time": 0.1
                    }
                
                mock_client.compile_workflow.side_effect = mock_compile_response
                
                for i, invalid_dsl in enumerate(invalid_yaml_cases):
                    try:
                        result = await server.call_tool("compile_workflow", {
                            "dsl_code": invalid_dsl,
                            "name": f"invalid_test_{i}"
                        })
                        
                        # Should detect and report invalid YAML
                        if result:
                            if result.get("isError", False):
                                print(f"‚úÖ Invalid YAML case {i+1}: Error properly detected")
                            else:
                                # Check if response indicates compilation failure
                                content_str = str(result.get("content", "")).lower()
                                if "validation" in content_str or "failed" in content_str:
                                    print(f"‚úÖ Invalid YAML case {i+1}: Validation failure detected")
                                else:
                                    print(f"üìù Invalid YAML case {i+1}: Validation unclear")
                        else:
                            print(f"üìù Invalid YAML case {i+1}: No response received")
                            
                    except Exception as e:
                        print(f"‚úÖ Invalid YAML case {i+1}: Exception properly handled - {e}")

    @pytest.mark.asyncio
    async def test_missing_required_fields(self):
        """Test handling of workflows missing required fields."""
        missing_field_cases = [
            "",  # Empty DSL
            "description: test workflow",  # Missing name
            "name: test",  # Missing steps
            "name: test\nsteps: []",  # Empty steps
            "name: test\nsteps:\n  - run: echo 'test'",  # Missing step name
        ]
        
        async with mcp_test_server(debug=True) as server:
            with patch('kubiya_workflow_sdk.client.StreamingKubiyaClient') as mock_client_class:
                mock_client = AsyncMock()
                mock_client_class.return_value = mock_client
                
                # Configure mock to return field validation errors
                def mock_validation_response(dsl_code, **kwargs):
                    if not dsl_code.strip():
                        return {
                            "workflow_id": None,
                            "status": "failed",
                            "validation_errors": ["Empty DSL provided"],
                        }
                    elif "name:" not in dsl_code:
                        return {
                            "workflow_id": None,
                            "status": "failed", 
                            "validation_errors": ["Missing required field: name"],
                        }
                    elif "steps:" not in dsl_code:
                        return {
                            "workflow_id": None,
                            "status": "failed",
                            "validation_errors": ["Missing required field: steps"],
                        }
                    else:
                        return {
                            "workflow_id": None,
                            "status": "failed",
                            "validation_errors": ["Invalid workflow structure"],
                        }
                
                mock_client.compile_workflow.side_effect = mock_validation_response
                
                for i, incomplete_dsl in enumerate(missing_field_cases):
                    try:
                        result = await server.call_tool("compile_workflow", {
                            "dsl_code": incomplete_dsl,
                            "name": f"incomplete_test_{i}"
                        })
                        
                        # Should detect missing required fields
                        if result:
                            content_str = str(result).lower()
                            if ("validation" in content_str or "missing" in content_str or 
                                "error" in content_str or result.get("isError", False)):
                                print(f"‚úÖ Missing field case {i+1}: Validation error detected")
                            else:
                                print(f"üìù Missing field case {i+1}: Validation unclear")
                        
                    except Exception as e:
                        print(f"‚úÖ Missing field case {i+1}: Exception handled - {e}")

    @pytest.mark.asyncio
    async def test_malformed_json_parameters(self):
        """Test handling of malformed JSON parameters."""
        malformed_json_cases = [
            '{"invalid": json, "structure"}',
            '{"unclosed": "json"',
            '{invalid json structure}',
            '{"nested": {"invalid": json}}',
            '[invalid, json, array]',
        ]
        
        async with mcp_test_server(debug=True) as server:
            with patch('kubiya_workflow_sdk.client.StreamingKubiyaClient') as mock_client_class:
                mock_client = AsyncMock()
                mock_client_class.return_value = mock_client
                
                # Mock should handle JSON parsing errors
                mock_client.execute_workflow.return_value = {
                    "execution_id": "json_test",
                    "status": "failed",
                    "error": "Invalid JSON in parameters"
                }
                
                for i, malformed_json in enumerate(malformed_json_cases):
                    try:
                        result = await server.call_tool("execute_workflow", {
                            "workflow_input": WorkflowTestData.SIMPLE_WORKFLOWS["hello_world"]["dsl"],
                            "params": malformed_json  # This should be parsed as JSON
                        })
                        
                        # Should handle malformed JSON gracefully
                        if result:
                            if result.get("isError", False) or "error" in str(result).lower():
                                print(f"‚úÖ Malformed JSON case {i+1}: Error properly handled")
                            else:
                                print(f"üìù Malformed JSON case {i+1}: Handling unclear")
                                
                    except Exception as e:
                        print(f"‚úÖ Malformed JSON case {i+1}: Exception handled - {e}")

    @pytest.mark.asyncio
    async def test_extremely_large_workflow_definitions(self):
        """Test handling of extremely large workflow definitions."""
        # Generate large workflow DSL
        large_steps = []
        for i in range(100):  # 100 steps
            large_steps.append(f"  - name: step_{i}\n    run: echo 'Step {i}'")
        
        large_workflow_dsl = f"""
name: large_workflow_test
description: Test workflow with many steps
steps:
{chr(10).join(large_steps)}
"""
        
        async with mcp_test_server(debug=True) as server:
            with patch('kubiya_workflow_sdk.client.StreamingKubiyaClient') as mock_client_class:
                mock_client = AsyncMock()
                mock_client_class.return_value = mock_client
                
                # Mock response for large workflow
                mock_client.compile_workflow.return_value = {
                    "workflow_id": "large_wf_123",
                    "status": "compiled",
                    "validation_errors": [],
                    "compilation_time": 2.5,  # Longer compilation time
                    "manifest": {"name": "large_workflow_test", "steps": [{"name": f"step_{i}"} for i in range(100)]}
                }
                
                try:
                    start_time = time.time()
                    result = await server.call_tool("compile_workflow", {
                        "dsl_code": large_workflow_dsl,
                        "name": "large_workflow_test"
                    })
                    processing_time = time.time() - start_time
                    
                    # Should handle large workflows without hanging
                    if result:
                        if not result.get("isError", False):
                            print(f"‚úÖ Large workflow (100 steps) handled successfully in {processing_time:.3f}s")
                        else:
                            print(f"üìù Large workflow caused error: {result.get('content', '')}")
                    else:
                        print("üìù Large workflow resulted in no response")
                        
                except Exception as e:
                    print(f"‚úÖ Large workflow caused controlled exception: {e}")


class TestResourceConstraintHandling:
    """Test handling of resource constraints and limitations."""
    
    @pytest.mark.asyncio
    async def test_memory_exhaustion_scenarios(self):
        """Test behavior under memory exhaustion conditions."""
        async with mcp_test_server(debug=True) as server:
            with patch('kubiya_workflow_sdk.client.StreamingKubiyaClient') as mock_client_class:
                # Use resource exhaustion mock
                mock_client = EnhancedMockScenarios.create_resource_exhaustion_scenario()
                mock_client_class.return_value = mock_client
                
                try:
                    result = await server.call_tool("execute_workflow", {
                        "workflow_input": WorkflowTestData.SIMPLE_WORKFLOWS["hello_world"]["dsl"],
                        "memory_limit": "64MB"  # Very low memory limit
                    })
                    
                    # Should handle memory constraints
                    if result:
                        if result.get("isError", False) or "resource" in str(result).lower():
                            print("‚úÖ Memory exhaustion scenario handled appropriately")
                        else:
                            print("üìù Memory exhaustion handling unclear")
                    
                except Exception as e:
                    print(f"‚úÖ Memory exhaustion caused controlled exception: {e}")

    @pytest.mark.asyncio 
    async def test_disk_space_exhaustion(self):
        """Test behavior when disk space is exhausted."""
        async with mcp_test_server(debug=True) as server:
            with patch('kubiya_workflow_sdk.client.StreamingKubiyaClient') as mock_client_class:
                mock_client = AsyncMock()
                mock_client_class.return_value = mock_client
                
                # Simulate disk space exhaustion
                mock_client.execute_workflow.side_effect = OSError("No space left on device")
                
                try:
                    result = await server.call_tool("execute_workflow", {
                        "workflow_input": """
name: disk_test
steps:
  - name: create_large_file
    run: dd if=/dev/zero of=/tmp/large_file bs=1G count=10
"""
                    })
                    
                    # Should handle disk space issues
                    if result and result.get("isError", False):
                        print("‚úÖ Disk space exhaustion handled gracefully")
                    else:
                        print("üìù Disk space exhaustion not properly detected")
                        
                except Exception as e:
                    print(f"‚úÖ Disk space exhaustion caused controlled exception: {e}")

    @pytest.mark.asyncio
    async def test_cpu_saturation_handling(self):
        """Test behavior under CPU saturation conditions."""
        async with mcp_test_server(debug=True) as server:
            with patch('kubiya_workflow_sdk.client.StreamingKubiyaClient') as mock_client_class:
                mock_client = AsyncMock()
                mock_client_class.return_value = mock_client
                
                # Simulate CPU saturation delay
                async def cpu_saturated_response(*args, **kwargs):
                    await asyncio.sleep(1.0)  # Simulate slow response due to CPU load
                    return {
                        "execution_id": "cpu_test",
                        "status": "completed",
                        "duration": 5.0,  # Longer than normal
                        "resource_usage": {"cpu_percent": 99.5}
                    }
                
                mock_client.execute_workflow.side_effect = cpu_saturated_response
                
                try:
                    start_time = time.time()
                    result = await server.call_tool("execute_workflow", {
                        "workflow_input": """
name: cpu_intensive
steps:
  - name: cpu_load
    run: python3 -c "while True: pass"  # Infinite CPU loop
"""
                    })
                    response_time = time.time() - start_time
                    
                    # Should handle CPU saturation
                    if result and response_time > 0.5:  # Slower response expected
                        print(f"‚úÖ CPU saturation handled (response time: {response_time:.3f}s)")
                    else:
                        print("üìù CPU saturation handling unclear")
                        
                except Exception as e:
                    print(f"‚úÖ CPU saturation caused controlled exception: {e}")


class TestConcurrentAccessHandling:
    """Test concurrent access scenarios and rate limiting."""
    
    @pytest.mark.asyncio
    async def test_concurrent_workflow_execution(self):
        """Test handling of multiple concurrent workflow executions."""
        async with mcp_test_server(debug=True) as server:
            with patch('kubiya_workflow_sdk.client.StreamingKubiyaClient') as mock_client_class:
                mock_client = StatefulMockClient()
                mock_client_class.return_value = mock_client
                
                # Execute multiple workflows concurrently
                concurrent_tasks = []
                for i in range(5):
                    task = server.call_tool("execute_workflow", {
                        "workflow_input": f"""
name: concurrent_test_{i}
steps:
  - name: concurrent_step
    run: echo "Concurrent execution {i}"
""",
                        "execution_id": f"concurrent_{i}"
                    })
                    concurrent_tasks.append(task)
                
                try:
                    start_time = time.time()
                    results = await asyncio.gather(*concurrent_tasks, return_exceptions=True)
                    execution_time = time.time() - start_time
                    
                    # Analyze concurrent execution results
                    success_count = 0
                    error_count = 0
                    
                    for i, result in enumerate(results):
                        if isinstance(result, Exception):
                            error_count += 1
                            print(f"üìù Concurrent task {i}: Exception - {result}")
                        elif result and not result.get("isError", False):
                            success_count += 1
                        else:
                            error_count += 1
                    
                    print(f"‚úÖ Concurrent execution: {success_count} successes, {error_count} errors in {execution_time:.3f}s")
                    
                    # Should handle some level of concurrency
                    assert success_count + error_count == 5
                    
                except Exception as e:
                    print(f"üìù Concurrent execution test: {e}")

    @pytest.mark.asyncio
    async def test_rate_limiting_behavior(self):
        """Test rate limiting and request throttling."""
        async with mcp_test_server(debug=True) as server:
            with patch('kubiya_workflow_sdk.client.StreamingKubiyaClient') as mock_client_class:
                mock_client = AsyncMock()
                mock_client_class.return_value = mock_client
                
                # Simulate rate limiting after too many requests
                request_count = 0
                def rate_limited_response(*args, **kwargs):
                    nonlocal request_count
                    request_count += 1
                    if request_count > 3:  # Rate limit after 3 requests
                        raise Exception("429 Too Many Requests")
                    return {"runners": []}
                
                mock_client.get_runners.side_effect = rate_limited_response
                
                rate_limited_count = 0
                success_count = 0
                
                # Make rapid requests to trigger rate limiting
                for i in range(6):
                    try:
                        result = await server.call_tool("get_workflow_runners", {
                            "refresh": True
                        })
                        
                        if result and not result.get("isError", False):
                            success_count += 1
                        
                    except Exception as e:
                        if "429" in str(e) or "rate" in str(e).lower():
                            rate_limited_count += 1
                        print(f"Request {i+1}: {e}")
                    
                    await asyncio.sleep(0.1)  # Small delay between requests
                
                print(f"‚úÖ Rate limiting test: {success_count} successes, {rate_limited_count} rate limited")
                
                # Should have some rate limiting behavior
                assert rate_limited_count > 0 or success_count < 6

    @pytest.mark.asyncio
    async def test_deadlock_prevention(self):
        """Test deadlock prevention in concurrent scenarios."""
        async with mcp_test_server(debug=True) as server:
            with patch('kubiya_workflow_sdk.client.StreamingKubiyaClient') as mock_client_class:
                mock_client = AsyncMock()
                mock_client_class.return_value = mock_client
                
                # Simulate potential deadlock scenario
                lock_acquired = False
                async def potentially_blocking_operation(*args, **kwargs):
                    nonlocal lock_acquired
                    if lock_acquired:
                        await asyncio.sleep(5.0)  # Long wait that could cause deadlock
                        return {"status": "completed_after_wait"}
                    else:
                        lock_acquired = True
                        await asyncio.sleep(0.1)
                        lock_acquired = False
                        return {"status": "completed_normally"}
                
                mock_client.compile_workflow.side_effect = potentially_blocking_operation
                
                # Execute operations that could deadlock
                tasks = []
                for i in range(3):
                    task = server.call_tool("compile_workflow", {
                        "dsl_code": f"name: deadlock_test_{i}\nsteps:\n  - name: step\n    run: echo 'test'"
                    })
                    tasks.append(task)
                
                try:
                    # Use timeout to prevent actual deadlock in test
                    results = await asyncio.wait_for(
                        asyncio.gather(*tasks, return_exceptions=True),
                        timeout=10.0
                    )
                    
                    completed_count = sum(1 for r in results if not isinstance(r, Exception))
                    print(f"‚úÖ Deadlock prevention: {completed_count}/{len(tasks)} operations completed")
                    
                except asyncio.TimeoutError:
                    print("üìù Potential deadlock detected - operations timed out")
                except Exception as e:
                    print(f"üìù Deadlock prevention test: {e}")


class TestTimeoutScenarios:
    """Test various timeout scenarios and handling."""
    
    @pytest.mark.asyncio
    async def test_workflow_execution_timeout(self):
        """Test handling of workflow execution timeouts."""
        async with mcp_test_server(debug=True) as server:
            with patch('kubiya_workflow_sdk.client.StreamingKubiyaClient') as mock_client_class:
                mock_client = AsyncMock()
                mock_client_class.return_value = mock_client
                
                # Simulate long-running workflow that times out
                async def long_running_workflow(*args, **kwargs):
                    await asyncio.sleep(5.0)  # Simulate long execution
                    return {
                        "execution_id": "timeout_test",
                        "status": "timeout",
                        "error": "Execution timed out after 30 seconds"
                    }
                
                mock_client.execute_workflow.side_effect = long_running_workflow
                
                try:
                    # Set a shorter timeout for testing
                    result = await asyncio.wait_for(
                        server.call_tool("execute_workflow", {
                            "workflow_input": """
name: long_running_test
steps:
  - name: long_step
    run: sleep 60  # Very long running command
""",
                            "timeout": 30
                        }),
                        timeout=2.0  # Test timeout
                    )
                    
                    if result and ("timeout" in str(result).lower() or result.get("isError", False)):
                        print("‚úÖ Workflow execution timeout handled properly")
                    else:
                        print("üìù Timeout handling unclear")
                        
                except asyncio.TimeoutError:
                    print("‚úÖ Workflow execution timeout detected and handled")
                except Exception as e:
                    print(f"‚úÖ Timeout caused controlled exception: {e}")

    @pytest.mark.asyncio
    async def test_compilation_timeout(self):
        """Test handling of workflow compilation timeouts."""
        async with mcp_test_server(debug=True) as server:
            with patch('kubiya_workflow_sdk.client.StreamingKubiyaClient') as mock_client_class:
                mock_client = AsyncMock()
                mock_client_class.return_value = mock_client
                
                # Simulate slow compilation
                async def slow_compilation(*args, **kwargs):
                    await asyncio.sleep(3.0)
                    return {
                        "workflow_id": "slow_compile",
                        "status": "compiled",
                        "compilation_time": 30.0
                    }
                
                mock_client.compile_workflow.side_effect = slow_compilation
                
                try:
                    result = await asyncio.wait_for(
                        server.call_tool("compile_workflow", {
                            "dsl_code": WorkflowTestData.COMPLEX_WORKFLOWS["ci_cd_pipeline"]["dsl"]
                        }),
                        timeout=1.0  # Short timeout for testing
                    )
                    
                    print("üìù Compilation completed within timeout")
                    
                except asyncio.TimeoutError:
                    print("‚úÖ Compilation timeout detected and handled")
                except Exception as e:
                    print(f"‚úÖ Compilation timeout caused controlled exception: {e}")

    @pytest.mark.asyncio
    async def test_api_response_timeout(self):
        """Test handling of API response timeouts."""
        async with mcp_test_server(debug=True) as server:
            with patch('kubiya_workflow_sdk.client.StreamingKubiyaClient') as mock_client_class:
                mock_client = AsyncMock()
                mock_client_class.return_value = mock_client
                
                # Simulate API that never responds
                async def never_responds(*args, **kwargs):
                    await asyncio.sleep(60)  # Very long delay
                    return {"runners": []}
                
                mock_client.get_runners.side_effect = never_responds
                
                try:
                    result = await asyncio.wait_for(
                        server.call_tool("get_workflow_runners", {}),
                        timeout=1.0
                    )
                    
                    print("üìù API response received within timeout")
                    
                except asyncio.TimeoutError:
                    print("‚úÖ API response timeout detected and handled")
                except Exception as e:
                    print(f"‚úÖ API timeout caused controlled exception: {e}")


class TestEdgeCaseInputHandling:
    """Test edge cases in input handling and validation."""
    
    @pytest.mark.asyncio
    async def test_unicode_and_special_characters(self):
        """Test handling of Unicode and special characters in inputs."""
        unicode_test_cases = [
            "ÊµãËØïÂ∑•‰ΩúÊµÅ",  # Chinese characters
            "—Ç–µ—Å—Ç —Ä–∞–±–æ—á–µ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞",  # Cyrillic characters
            "üöÄ rocket workflow üéØ",  # Emojis
            "caf√©_na√Øve_r√©sum√©",  # Accented characters
            "\\n\\t\\r special chars",  # Escape sequences
            "workflow with\nnewlines\tand\ttabs",
        ]
        
        async with mcp_test_server(debug=True) as server:
            with patch('kubiya_workflow_sdk.client.StreamingKubiyaClient') as mock_client_class:
                mock_client = AsyncMock()
                mock_client_class.return_value = mock_client
                
                mock_client.compile_workflow.return_value = {
                    "workflow_id": "unicode_test",
                    "status": "compiled",
                    "validation_errors": []
                }
                
                for i, unicode_name in enumerate(unicode_test_cases):
                    try:
                        result = await server.call_tool("compile_workflow", {
                            "dsl_code": f"""
name: {unicode_name}
description: Unicode test workflow
steps:
  - name: unicode_step
    run: echo "Testing Unicode: {unicode_name}"
""",
                            "name": unicode_name
                        })
                        
                        # Should handle Unicode gracefully
                        if result and not result.get("isError", False):
                            print(f"‚úÖ Unicode case {i+1}: Handled successfully")
                        else:
                            print(f"üìù Unicode case {i+1}: Handling unclear")
                            
                    except Exception as e:
                        print(f"‚úÖ Unicode case {i+1}: Exception handled - {e}")

    @pytest.mark.asyncio
    async def test_extremely_long_strings(self):
        """Test handling of extremely long string inputs."""
        # Generate very long strings
        long_name = "a" * 10000  # 10KB name
        long_description = "b" * 100000  # 100KB description
        
        async with mcp_test_server(debug=True) as server:
            with patch('kubiya_workflow_sdk.client.StreamingKubiyaClient') as mock_client_class:
                mock_client = AsyncMock()
                mock_client_class.return_value = mock_client
                
                # Mock should handle or reject very long inputs
                def handle_long_input(dsl_code, **kwargs):
                    if len(dsl_code) > 50000:  # Very large DSL
                        return {
                            "workflow_id": None,
                            "status": "failed",
                            "validation_errors": ["DSL too large"]
                        }
                    return {
                        "workflow_id": "long_input_test",
                        "status": "compiled"
                    }
                
                mock_client.compile_workflow.side_effect = handle_long_input
                
                try:
                    result = await server.call_tool("compile_workflow", {
                        "dsl_code": f"""
name: {long_name}
description: {long_description}
steps:
  - name: test_step
    run: echo 'Long input test'
""",
                        "name": long_name[:100]  # Truncate for parameter
                    })
                    
                    # Should handle long inputs appropriately
                    if result:
                        if result.get("isError", False) or "too large" in str(result).lower():
                            print("‚úÖ Extremely long string input properly rejected")
                        else:
                            print("‚úÖ Extremely long string input handled successfully")
                    
                except Exception as e:
                    print(f"‚úÖ Long string input caused controlled exception: {e}")

    @pytest.mark.asyncio
    async def test_null_and_empty_inputs(self):
        """Test handling of null and empty inputs."""
        null_empty_cases = [
            {"dsl_code": "", "name": "empty_dsl"},
            {"dsl_code": None, "name": "null_dsl"},
            {"dsl_code": "name: test\nsteps: []", "name": ""},
            {"dsl_code": "name: test\nsteps: []", "name": None},
            {"dsl_code": "   \n  \t  \n", "name": "whitespace_only"},
        ]
        
        async with mcp_test_server(debug=True) as server:
            with patch('kubiya_workflow_sdk.client.StreamingKubiyaClient') as mock_client_class:
                mock_client = AsyncMock()
                mock_client_class.return_value = mock_client
                
                # Mock should handle null/empty inputs
                def handle_null_empty(dsl_code, **kwargs):
                    if not dsl_code or not dsl_code.strip():
                        return {
                            "workflow_id": None,
                            "status": "failed",
                            "validation_errors": ["Empty or null DSL provided"]
                        }
                    return {
                        "workflow_id": "null_empty_test",
                        "status": "compiled"
                    }
                
                mock_client.compile_workflow.side_effect = handle_null_empty
                
                for i, test_case in enumerate(null_empty_cases):
                    try:
                        result = await server.call_tool("compile_workflow", test_case)
                        
                        # Should handle null/empty inputs gracefully
                        if result:
                            if result.get("isError", False) or "validation" in str(result).lower():
                                print(f"‚úÖ Null/empty case {i+1}: Validation error detected")
                            else:
                                print(f"üìù Null/empty case {i+1}: Handling unclear")
                        
                    except Exception as e:
                        print(f"‚úÖ Null/empty case {i+1}: Exception handled - {e}")


# Test runner for validation
if __name__ == "__main__":
    print("Comprehensive error handling and edge case tests loaded successfully")
    print("Test classes available:")
    print("- TestNetworkFailureHandling")
    print("- TestInvalidWorkflowHandling") 
    print("- TestResourceConstraintHandling")
    print("- TestConcurrentAccessHandling")
    print("- TestTimeoutScenarios")
    print("- TestEdgeCaseInputHandling")