---
title: "Advanced Workflows"
description: "Deep dive into advanced workflow features, streaming, custom providers, and production deployment"
icon: "rocket"
---

# Advanced Workflows

Master the advanced features of Kubiya workflows for production-grade automation.

## Advanced Step Types

### 1. **Shell Steps**

Execute shell commands in any container:

```python
from kubiya_workflow_sdk import step

result = step("process-data").shell(
    image="alpine:latest",
    command="""
    echo "Processing..."
    cat input.json | jq '.data[]' > output.json
    """,
    env={"API_KEY": "${secrets.API_KEY}"}
)
```

### 2. **Python Steps**

Run Python scripts with full ecosystem:

```python
from kubiya_workflow_sdk import step

analysis = step(
    name="analyze-metrics",
    packages=["pandas", "numpy", "scikit-learn"],
    image="python:3.11-slim",
    volumes={"/data": "metrics-volume"}
).python(
    script="""
import pandas as pd
import numpy as np

df = pd.read_csv('/data/metrics.csv')
summary = df.describe()
print(summary.to_json())
    """
)
```

### 3. **Container Steps**

Run any containerized application:

```python
from kubiya_workflow_sdk import step

server = step(
    name="web-server",
    ports={"80": "8080"},
    volumes={
        "./config": "/etc/nginx/conf.d",
        "./static": "/usr/share/nginx/html"
    },
    health_check={
        "test": ["CMD", "curl", "-f", "http://localhost/health"],
        "interval": "30s",
        "retries": 3
    }
).docker(
    image="nginx:alpine",
)
```

### 4. **Inline Agent Steps**

Embed AI decision-making:

```python
from kubiya_workflow_sdk import step

decision = step(
    name="deployment-decision",
).inline_agent(
    agent_name="deployment-decision",
    ai_instructions="You are an AI assistant that decides whether to deploy based on metrics.",
    message="Analyze these metrics and decide if we should deploy: ${metrics}",
    runners=["kubiya-hosted"],
    llm_model="gpt-4",
    is_debug_mode=True,
    tools=[
        {
            "name": "check-metrics",
            "type": "docker",
            "image": "datadog/agent:latest",
            "content": "datadog-check.sh",
            "args": {"threshold": "0.95"}
        }
    ]
)
```

## Streaming and Real-Time Updates

### SSE Streaming

```python
import json
from kubiya_workflow_sdk import KubiyaClient

client = KubiyaClient(api_key="your-key") # Replace with your actual API key

# Stream workflow execution
for event_str in client.execute_workflow(workflow, stream=True):
    try:
        event = json.loads(event_str)
        if event.get("type") == "step_running":
            print(f"▶️  Starting: {event['step'].get('name')}")
        elif event.get("type") == "step_completed":
            print(f"✅ Completed: {event['step'].get('name')}")
        elif event.get("type") == "log":
            print(f"   Output: {event.get('message')}")
    except json.JSONDecodeError:
        # Handle raw string events
        print(event_str)

```

### Custom Stream Processing

```python
import json
from kubiya_workflow_sdk import KubiyaClient
from kubiya_workflow_sdk.dsl import Workflow

class WorkflowStreamProcessor:
    def __init__(self, kubiya_client: KubiyaClient):
        self.steps_completed = 0
        self.logs = []
        self.__kubiya_client = kubiya_client

    def process_stream(self, workflow: Workflow):
        for event in self.__kubiya_client.execute_workflow(workflow.to_dict()):
            event = json.loads(event)
            if event.get("type") == "step_completed":
                self.steps_completed += 1
                self.notify_progress(self.steps_completed)
            elif event.get("type") == "log":
                self.logs.append(event)
                self.send_to_monitoring(event)

```

## Custom Providers

### Creating a Custom Provider

```python
from kubiya_workflow_sdk.providers import BaseProvider
from typing import Dict, Any, AsyncGenerator

class CustomLLMProvider(BaseProvider):
    """Custom provider for your LLM"""
    
    def __init__(self, api_key: str, model: str = "custom-model"):
        self.api_key = api_key
        self.model = model
        
    async def compose(
        self, 
        task: str,
        mode: str = "plan",
        context: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """Generate workflow from natural language"""
        # Your LLM logic here
        workflow = await self._call_llm(task, context)
        return {
            "workflow": workflow,
            "metadata": {"model": self.model}
        }
        
    async def stream_compose(
        self,
        task: str,
        mode: str = "plan"
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """Stream workflow generation"""
        async for chunk in self._stream_llm(task):
            yield {
                "type": "token",
                "content": chunk
            }
```

### Registering Custom Provider

```python
from kubiya_workflow_sdk.providers import register_provider

# Register your provider
register_provider("custom-llm", CustomLLMProvider)

# Use it
provider = get_provider("custom-llm", api_key="...")
workflow = await provider.compose("Deploy my app")
```

## Kubernetes Deployment
Simply create a local runner on the Kubiya platform web interface, REST API, or CLI to get a manifest, give your runner a name - and deploy it on your cluster
-> You can now reference this runner string for workflow execution


## Next Steps

<CardGroup cols={2}>
  <Card title="API Reference" icon="code" href="/api-reference/compose">
    Complete API documentation
  </Card>
  <Card title="Examples" icon="lightbulb" href="/workflows/examples">
    Real-world workflow patterns
  </Card>
  <Card title="Troubleshooting" icon="wrench" href="/troubleshooting">
    Common issues and solutions
  </Card>
</CardGroup> 